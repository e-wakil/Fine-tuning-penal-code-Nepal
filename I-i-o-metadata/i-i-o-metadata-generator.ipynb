{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14384382,"sourceType":"datasetVersion","datasetId":9186512}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#/kaggle/input/asasasaa/final_legal_laws_metadata.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:44.604094Z","iopub.execute_input":"2026-01-16T11:34:44.604366Z","iopub.status.idle":"2026-01-16T11:34:44.608383Z","shell.execute_reply.started":"2026-01-16T11:34:44.604329Z","shell.execute_reply":"2026-01-16T11:34:44.607683Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import json\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\nimport random\n\n# ============================================\n# ============================================\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\nINPUT_JSON = \"/kaggle/input/asasasaa/final_legal_laws_metadata.json\"\nOUTPUT_JSON = \"/kaggle/working/nepal_legal_instruction_dataset.json\"\n\n# Generation parameters\nTEMPERATURE = 0.7\nMAX_NEW_TOKENS = 600\nTOP_P = 0.9\nDO_SAMPLE = True\nSAMPLES_PER_CHUNK = 4  # 4 samples per chunk\n\n# ============================================\n# 25 CONTROLLED INSTRUCTION TEMPLATES\n# ============================================\nINSTRUCTION_TEMPLATES = [\n    \"Explain the legal meaning of this provision in simple language.\",\n    \"Summarize the main points of this law in 2–3 sentences.\",\n    \"Identify the rights and obligations established by this section.\",\n    \"Clarify this clause for a non-lawyer audience.\",\n    \"Interpret how this provision would be applied in practice.\",\n    \"Provide a real-world example illustrating this law.\",\n    \"Highlight the exceptions and limitations of this provision.\",\n    \"Explain the historical or legislative context of this section.\",\n    \"Compare this provision with related laws or sections.\",\n    \"Analyze the consequences of violating this clause.\",\n    \"Reword this law in plain language suitable for general public understanding.\",\n    \"Identify key terms in this section and explain them.\",\n    \"Explain which offences are covered under this provision.\",\n    \"Describe any conditions under which this law does not apply.\",\n    \"Provide a hypothetical scenario showing how this law would operate.\",\n    \"Explain the responsibilities of public authorities under this clause.\",\n    \"Analyze the connection of this section with human rights principles.\",\n    \"Identify the punishments or legal outcomes defined by this provision.\",\n    \"Explain the scope and jurisdiction of this law.\",\n    \"Discuss the rationale or legal reasoning behind this section.\",\n    \"Describe how this law affects individuals, corporations, or government entities.\",\n    \"Explain the limitations on exercising rights (e.g., private defence, consent) under this clause.\",\n    \"Highlight any conditions that make an act excused or not an offence.\",\n    \"Identify the specific offences mentioned and classify them (e.g., grave, heinous, minor).\",\n    \"Analyze this provision in the context of Nepal's National Penal Code 2017 and related chapters.\"\n]\n\n# ============================================\n# DYNAMIC INPUT GENERATORS\n# ============================================\ndef generate_dynamic_inputs(chunk):\n    \"\"\"Generate 3 dynamic inputs based on the chunk metadata\"\"\"\n    \n    section_title = chunk.get(\"section_title\", \"this section\")\n    chapter_title = chunk.get(\"chapter_title\", \"this chapter\")\n    law_name = chunk.get(\"law\", \"this law\")\n    \n    dynamic_inputs = [\n        # Dynamic Input 1: Question format\n        f\"What does {law_name}, Section {chunk.get('section', '')} ({section_title}) state?\",\n        \n        # Dynamic Input 2: Contextual query\n        f\"In the context of {chapter_title}, how should '{section_title}' be understood?\",\n        \n        # Dynamic Input 3: Application query\n        f\"How does the provision on '{section_title}' apply to situations in Nepal under {law_name}?\"\n    ]\n    \n    return dynamic_inputs\n\n# ============================================\n# CHECK GPU AVAILABILITY\n# ============================================\nprint(\"=\"*70)\nprint(\"GPU CHECK\")\nprint(\"=\"*70)\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nprint(\"=\"*70)\n\n# ============================================\n# LOAD MODEL WITH GPU\n# ============================================\nprint(\"\\nLoading Mistral 7B model on GPU...\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True\n)\n\nmodel.eval()\nprint(\"✓ Model loaded successfully on GPU!\")\nprint(f\"✓ Model device: {model.device}\")\nprint(f\"✓ Model dtype: {model.dtype}\")\n\n# ============================================\n# GENERATION FUNCTION\n# ============================================\ndef generate_output(instruction, input_text, chunk):\n    \"\"\"Generate output using the model\"\"\"\n    \n    # Create context-aware prompt\n    prompt_template = f\"\"\"You are a legal expert on Nepal's National Penal Code 2017.\n\nLegal Context:\n- Law: {chunk.get('law', '')}\n- Chapter: {chunk.get('chapter_title', '')}\n- Section: {chunk.get('section_title', '')}\n- Legal Text: {chunk.get('text', '')}\n\nUser Query: {input_text}\n\nTask: {instruction}\n\nProvide a clear, accurate, and comprehensive response based on the legal provision above.\"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt_template}]\n    \n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    try:\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=TEMPERATURE,\n                top_p=TOP_P,\n                do_sample=DO_SAMPLE,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        assistant_response = generated_text[len(prompt):].strip()\n        \n        # Clean up the response\n        if not assistant_response or len(assistant_response) < 20:\n            assistant_response = f\"Based on {chunk.get('section_title', 'this provision')}, {chunk.get('text', '')[:200]}\"\n        \n        return assistant_response\n        \n    except Exception as e:\n        print(f\"Error generating output: {str(e)}\")\n        return f\"Error processing this legal provision. Please refer to the original text.\"\n\n# ============================================\n# PROCESS CHUNKS\n# ============================================\ndef process_legal_chunks(legal_chunks):\n    \"\"\"Process all chunks and generate 4 samples each\"\"\"\n    \n    instruction_dataset = []\n    total_chunks = len(legal_chunks)\n    \n    print(f\"\\nProcessing {total_chunks} legal chunks...\")\n    print(f\"Generating {SAMPLES_PER_CHUNK} samples per chunk = {total_chunks * SAMPLES_PER_CHUNK} total samples\")\n    print(\"=\"*70)\n    \n    for chunk_idx, chunk in enumerate(tqdm(legal_chunks, desc=\"Processing chunks\")):\n        \n        # Select 4 random instructions for this chunk\n        selected_instructions = random.sample(INSTRUCTION_TEMPLATES, SAMPLES_PER_CHUNK)\n        \n        # Generate dynamic inputs\n        dynamic_inputs = generate_dynamic_inputs(chunk)\n        \n        # Generate 4 samples\n        for sample_idx in range(SAMPLES_PER_CHUNK):\n            instruction = selected_instructions[sample_idx]\n            \n            # First sample uses original text as input\n            # Other 3 samples use dynamic inputs\n            if sample_idx == 0:\n                input_text = chunk.get(\"text\", \"\")\n            else:\n                input_text = dynamic_inputs[sample_idx - 1]\n            \n            # Generate output\n            output = generate_output(instruction, input_text, chunk)\n            \n            # Create dataset entry\n            dataset_entry = {\n                \"instruction\": instruction,\n                \"input\": input_text,\n                \"output\": output,\n                \"metadata\": {\n                    \"law\": chunk.get(\"law\", \"\"),\n                    \"part\": chunk.get(\"part\", \"\"),\n                    \"chapter\": chunk.get(\"chapter\", \"\"),\n                    \"chapter_title\": chunk.get(\"chapter_title\", \"\"),\n                    \"section\": chunk.get(\"section\", \"\"),\n                    \"section_title\": chunk.get(\"section_title\", \"\"),\n                    \"subsection\": chunk.get(\"subsection\", \"\"),\n                    \"chunk_id\": chunk.get(\"chunk_id\", \"\"),\n                    \"source\": chunk.get(\"source\", \"\"),\n                    \"original_text\": chunk.get(\"text\", \"\"),\n                    \"sample_type\": \"original_text_input\" if sample_idx == 0 else f\"dynamic_input_{sample_idx}\"\n                }\n            }\n            \n            instruction_dataset.append(dataset_entry)\n        \n        # Periodic checkpoint every 25 chunks\n        if (chunk_idx + 1) % 25 == 0:\n            with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n                json.dump(instruction_dataset, f, ensure_ascii=False, indent=2)\n            \n            if torch.cuda.is_available():\n                gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n                print(f\"\\n✓ Checkpoint: Processed {chunk_idx + 1}/{total_chunks} chunks\")\n                print(f\"  Generated: {len(instruction_dataset)} samples | GPU Memory: {gpu_memory:.2f} GB\")\n    \n    return instruction_dataset\n\n# ============================================\n# MAIN EXECUTION\n# ============================================\ndef main():\n    print(\"\\n\" + \"=\"*70)\n    print(\"NEPAL LEGAL INSTRUCTION DATASET GENERATOR\")\n    print(\"=\"*70)\n    \n    # Load input data\n    print(f\"\\nLoading input dataset from: {INPUT_JSON}\")\n    with open(INPUT_JSON, 'r', encoding='utf-8') as f:\n        legal_chunks = json.load(f)\n    \n    print(f\"✓ Loaded {len(legal_chunks)} legal chunks\")\n    \n    # Process all chunks\n    instruction_dataset = process_legal_chunks(legal_chunks)\n    \n    # Final save\n    print(f\"\\n{'='*70}\")\n    print(f\"Saving final dataset to: {OUTPUT_JSON}\")\n    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n        json.dump(instruction_dataset, f, ensure_ascii=False, indent=2)\n    \n    print(f\"✓ COMPLETE! Generated {len(instruction_dataset)} instruction-output pairs\")\n    print(f\"✓ Total chunks processed: {len(legal_chunks)}\")\n    print(f\"✓ Samples per chunk: {SAMPLES_PER_CHUNK}\")\n    print(\"=\"*70)\n    \n    # Display statistics\n    print(\"\\nDATASET STATISTICS:\")\n    print(\"=\"*70)\n    original_text_samples = sum(1 for item in instruction_dataset if item['metadata']['sample_type'] == 'original_text_input')\n    dynamic_samples = len(instruction_dataset) - original_text_samples\n    \n    print(f\"Original text input samples: {original_text_samples}\")\n    print(f\"Dynamic input samples: {dynamic_samples}\")\n    print(f\"Total samples: {len(instruction_dataset)}\")\n    \n    # Display sample outputs\n    if instruction_dataset:\n        print(\"\\n\" + \"=\"*70)\n        print(\"SAMPLE OUTPUTS (First 2 samples):\")\n        print(\"=\"*70)\n        \n        for i, sample in enumerate(instruction_dataset[:2]):\n            print(f\"\\n--- SAMPLE {i+1} ---\")\n            print(f\"Instruction: {sample['instruction']}\")\n            print(f\"\\nInput: {sample['input'][:150]}...\")\n            print(f\"\\nOutput: {sample['output'][:300]}...\")\n            print(f\"\\nMetadata:\")\n            print(f\"  - Law: {sample['metadata']['law']}\")\n            print(f\"  - Section: {sample['metadata']['section_title']}\")\n            print(f\"  - Sample Type: {sample['metadata']['sample_type']}\")\n            print(\"-\"*70)\n    \n    # Final GPU stats\n    if torch.cuda.is_available():\n        print(f\"\\n✓ Peak GPU Memory Used: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"GENERATION COMPLETE!\")\n    print(\"=\"*70)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:44.714620Z","iopub.execute_input":"2026-01-16T11:34:44.714890Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nGPU CHECK\n======================================================================\nCUDA Available: True\nGPU Device: Tesla P100-PCIE-16GB\nGPU Memory: 17.06 GB\n======================================================================\n\nLoading Mistral 7B model on GPU...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21bece182b7b4061818dec5937c02b57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b4f9997b7440fe9541057dde853aa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf108a3ecb7a486499492d5e5b0bb3de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13dded7dd6bb40e98e9c9fb010cc7956"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b290a6a547a43e2bd2d8115539f8600"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2026-01-16 11:35:03.924004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768563304.103924      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768563304.156567      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768563304.580678      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768563304.580718      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768563304.580721      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768563304.580723      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d678c186f34339ad34011157a072ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76afc1edab8649f090d077a0a97fe3f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6227d096fe4180a2cd1cbd5ad16747"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd983b22de84c24a75f611e0c6cfdcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef0d0d12dcb3411bbcb10210412ee0e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ce26c0eaa84c4a89b9d11b6f679a7d"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded successfully on GPU!\n✓ Model device: cuda:0\n✓ Model dtype: torch.float16\n\n======================================================================\nNEPAL LEGAL INSTRUCTION DATASET GENERATOR\n======================================================================\n\nLoading input dataset from: /kaggle/input/asasasaa/final_legal_laws_metadata.json\n✓ Loaded 731 legal chunks\n\nProcessing 731 legal chunks...\nGenerating 4 samples per chunk = 2924 total samples\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Processing chunks:   0%|          | 0/731 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}